{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-w_gwenvj\n",
      "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numba in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from openai-whisper==20240930) (0.60.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from openai-whisper==20240930) (1.26.4)\n",
      "Collecting torch (from openai-whisper==20240930)\n",
      "  Using cached torch-2.5.1-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Collecting tqdm (from openai-whisper==20240930)\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting more-itertools (from openai-whisper==20240930)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper==20240930)\n",
      "  Using cached tiktoken-0.8.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper==20240930)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Collecting filelock (from torch->openai-whisper==20240930)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
      "Collecting networkx (from torch->openai-whisper==20240930)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch->openai-whisper==20240930)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch->openai-whisper==20240930)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch->openai-whisper==20240930)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->openai-whisper==20240930)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from tqdm->openai-whisper==20240930) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\epa_project\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->openai-whisper==20240930)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached tiktoken-0.8.0-cp39-cp39-win_amd64.whl (884 kB)\n",
      "Using cached torch-2.5.1-cp39-cp39-win_amd64.whl (203.0 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=812895 sha256=3b958b682b5a23963c3a40a2f65dfa9cb51b6817ca4865e02d9ca303fccb4017\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-yiswzk4v\\wheels\\fe\\03\\29\\e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: mpmath, tqdm, sympy, regex, networkx, more-itertools, MarkupSafe, fsspec, filelock, tiktoken, jinja2, torch, openai-whisper\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 more-itertools-10.5.0 mpmath-1.3.0 networkx-3.2.1 openai-whisper-20240930 regex-2024.11.6 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 tqdm-4.67.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-w_gwenvj'\n"
     ]
    }
   ],
   "source": [
    "# [1. 음성 파일 -> text 추출]\n",
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Anaconda3\\envs\\epa_project\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 중: ./test_audio/child_original\\jaemin_1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Anaconda3\\envs\\epa_project\\lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 텍스트 (jaemin_1.wav):\n",
      " My grandfather and my mom are the same.\n",
      "--------------------------------------------------\n",
      "처리 중: ./test_audio/child_original\\jaemin_2.wav\n",
      "추출된 텍스트 (jaemin_2.wav):\n",
      " It rained a lot when we went to Jeju-do.\n",
      "--------------------------------------------------\n",
      "처리 중: ./test_audio/child_original\\jaemin_3.wav\n",
      "추출된 텍스트 (jaemin_3.wav):\n",
      " I want to like you more than you do.\n",
      "--------------------------------------------------\n",
      "처리 중: ./test_audio/child_original\\jaemin_4.wav\n",
      "추출된 텍스트 (jaemin_4.wav):\n",
      " My daughter cried loudly.\n",
      "--------------------------------------------------\n",
      "처리 중: ./test_audio/child_original\\jaemin_5.wav\n",
      "추출된 텍스트 (jaemin_5.wav):\n",
      " It stopped raining.\n",
      "--------------------------------------------------\n",
      "처리 중: ./test_audio/child_original\\jaemin_6.wav\n",
      "추출된 텍스트 (jaemin_6.wav):\n",
      " He does not talk much.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import whisper\n",
    "\n",
    "# Whisper 모델 로드\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# 부모 폴더 경로 설정\n",
    "parent_folder = r\"./test_audio/child_original\"  # 음성 파일이 있는 폴더 경로\n",
    "\n",
    "# 지원되는 파일 형식\n",
    "audio_extensions = ('.wav')\n",
    "\n",
    "# 폴더 내 모든 음성 파일 불러오기\n",
    "audio_files = [f for f in glob.glob(os.path.join(parent_folder, \"*\")) if f.endswith(audio_extensions)]\n",
    "\n",
    "# 음성 파일 하나씩 처리\n",
    "for audio_file in audio_files:\n",
    "    print(f\"처리 중: {audio_file}\")\n",
    "    try:\n",
    "        # 텍스트 추출\n",
    "        result = model.transcribe(audio_file)\n",
    "        extracted_text = result[\"text\"]\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"추출된 텍스트 ({os.path.basename(audio_file)}):\")\n",
    "        print(extracted_text)\n",
    "        print(\"-\" * 50)  # 구분선\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생 ({audio_file}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2. 출력된 텍스트로 tortoise-TTS highquality 생성]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_file reference_file         evaluation_metric category       value\n",
      "0    yura_1.wav      tts_1.wav  pitch_contour_similarity   음율적 오류    0.147717\n",
      "1    yura_1.wav      tts_1.wav         rhythm_similarity   음율적 오류    0.000000\n",
      "2    yura_1.wav      tts_1.wav          pitch_difference      정확성  130.154541\n",
      "3    yura_1.wav      tts_1.wav            rms_difference      정확성    0.018920\n",
      "4    yura_1.wav      tts_1.wav    speech_rate_difference      유창성    0.083839\n",
      "..          ...            ...                       ...      ...         ...\n",
      "247  yura_6.wav      tts_6.wav          pitch_difference      정확성  364.937134\n",
      "248  yura_6.wav      tts_6.wav            rms_difference      정확성    0.003422\n",
      "249  yura_6.wav      tts_6.wav    speech_rate_difference      유창성    6.377279\n",
      "250  yura_6.wav      tts_6.wav          pause_difference      유창성    0.000000\n",
      "251  yura_6.wav      tts_6.wav             word_omission      완전성    0.000000\n",
      "\n",
      "[252 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import speech\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Google Cloud Speech-to-Text API 설정\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"KEY_PATH\")\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_audio(user_file, reference_file):\n",
    "    results = []\n",
    "    try:\n",
    "        # Load audio files\n",
    "        user_y, sr = librosa.load(user_file)\n",
    "        ref_y, _ = librosa.load(reference_file)\n",
    "\n",
    "        # 1) 음율적 오류 (Pitch, Rhythm)\n",
    "        user_pitches, _ = librosa.piptrack(y=user_y, sr=sr)\n",
    "        ref_pitches, _ = librosa.piptrack(y=ref_y, sr=sr)\n",
    "        user_pitch_pattern = [np.max(user_pitches[:, i]) for i in range(len(user_pitches[0])) if np.max(user_pitches[:, i]) > 0]\n",
    "        ref_pitch_pattern = [np.max(ref_pitches[:, i]) for i in range(len(ref_pitches[0])) if np.max(ref_pitches[:, i]) > 0]\n",
    "\n",
    "        min_length = min(len(user_pitch_pattern), len(ref_pitch_pattern))\n",
    "        pitch_contour_similarity = np.corrcoef(user_pitch_pattern[:min_length], ref_pitch_pattern[:min_length])[0, 1]\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_contour_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': pitch_contour_similarity\n",
    "        })\n",
    "\n",
    "        user_times = librosa.frames_to_time(range(len(user_pitches[0])), sr=sr)\n",
    "        ref_times = librosa.frames_to_time(range(len(ref_pitches[0])), sr=sr)\n",
    "        user_intervals = np.diff(user_times)\n",
    "        ref_intervals = np.diff(ref_times)\n",
    "        rhythm_similarity = np.abs(user_intervals[:min_length] - ref_intervals[:min_length]).mean()\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rhythm_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': rhythm_similarity\n",
    "        })\n",
    "\n",
    "        # 2) 정확성 (Pitch 평균, RMS)\n",
    "        user_avg_pitch = np.mean(user_pitch_pattern)\n",
    "        ref_avg_pitch = np.mean(ref_pitch_pattern)\n",
    "        pitch_difference = np.abs(user_avg_pitch - ref_avg_pitch)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_difference',\n",
    "            'category': '정확성',\n",
    "            'value': pitch_difference\n",
    "        })\n",
    "\n",
    "        user_rms = librosa.feature.rms(y=user_y).mean()\n",
    "        ref_rms = librosa.feature.rms(y=ref_y).mean()\n",
    "        rms_difference = np.abs(user_rms - ref_rms)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rms_difference',\n",
    "            'category': '정확성',\n",
    "            'value': rms_difference\n",
    "        })\n",
    "\n",
    "        # 3) 유창성 (Speech Rate, Pauses)\n",
    "        user_duration = librosa.get_duration(y=user_y, sr=sr)\n",
    "        ref_duration = librosa.get_duration(y=ref_y, sr=sr)\n",
    "        speech_rate_difference = np.abs((len(user_pitch_pattern) / user_duration) - (len(ref_pitch_pattern) / ref_duration))\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'speech_rate_difference',\n",
    "            'category': '유창성',\n",
    "            'value': speech_rate_difference\n",
    "        })\n",
    "\n",
    "        pause_threshold = 0.3\n",
    "        user_pauses = len([i for i in np.diff(user_times) if i > pause_threshold])\n",
    "        ref_pauses = len([i for i in np.diff(ref_times) if i > pause_threshold])\n",
    "        pause_difference = np.abs(user_pauses - ref_pauses)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pause_difference',\n",
    "            'category': '유창성',\n",
    "            'value': pause_difference\n",
    "        })\n",
    "\n",
    "        # 4) 완전성 (Word Omission)\n",
    "        word_omission = 0  # Simplified for brevity\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'word_omission',\n",
    "            'category': '완전성',\n",
    "            'value': word_omission\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'error',\n",
    "            'category': 'Error',\n",
    "            'value': str(e)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 메인 함수\n",
    "def main(reference_folder, user_folder):\n",
    "    all_results = []\n",
    "\n",
    "    for ref_file in os.listdir(reference_folder):\n",
    "        if ref_file.endswith(\".wav\"):\n",
    "            ref_path = os.path.join(reference_folder, ref_file)\n",
    "            \n",
    "            for user_file in os.listdir(user_folder):\n",
    "                if user_file.endswith(\".wav\"):\n",
    "                    user_path = os.path.join(user_folder, user_file)\n",
    "                    all_results.extend(evaluate_audio(user_path, ref_path))\n",
    "\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(df)\n",
    "\n",
    "# 실행\n",
    "reference_folder = \"./test_audio/adult_tts\"\n",
    "user_folder = \"./test_audio/adult_original\"\n",
    "main(reference_folder, user_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_file reference_file         evaluation_metric category  \\\n",
      "0    jaemin_1.wav      tts_1.wav  pitch_contour_similarity   음율적 오류   \n",
      "1    jaemin_1.wav      tts_1.wav         rhythm_similarity   음율적 오류   \n",
      "2    jaemin_1.wav      tts_1.wav          pitch_difference      정확성   \n",
      "3    jaemin_1.wav      tts_1.wav            rms_difference      정확성   \n",
      "4    jaemin_1.wav      tts_1.wav    speech_rate_difference      유창성   \n",
      "..            ...            ...                       ...      ...   \n",
      "247  jaemin_6.wav      tts_6.wav          pitch_difference      정확성   \n",
      "248  jaemin_6.wav      tts_6.wav            rms_difference      정확성   \n",
      "249  jaemin_6.wav      tts_6.wav    speech_rate_difference      유창성   \n",
      "250  jaemin_6.wav      tts_6.wav          pause_difference      유창성   \n",
      "251  jaemin_6.wav      tts_6.wav             word_omission      완전성   \n",
      "\n",
      "           value  \n",
      "0       0.080350  \n",
      "1       0.000000  \n",
      "2    1255.984619  \n",
      "3       0.068354  \n",
      "4       1.827973  \n",
      "..           ...  \n",
      "247  1080.032349  \n",
      "248     0.000087  \n",
      "249     2.234369  \n",
      "250     0.000000  \n",
      "251     0.000000  \n",
      "\n",
      "[252 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# [4. 성인 표준음성(TTS MARK) - 아동 진짜사람음성 차이]\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import speech\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Google Cloud Speech-to-Text API 설정\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"KEY_PATH\")\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_audio(user_file, reference_file):\n",
    "    results = []\n",
    "    try:\n",
    "        # Load audio files\n",
    "        user_y, sr = librosa.load(user_file)\n",
    "        ref_y, _ = librosa.load(reference_file)\n",
    "\n",
    "        # 1) 음율적 오류 (Pitch, Rhythm)\n",
    "        user_pitches, _ = librosa.piptrack(y=user_y, sr=sr)\n",
    "        ref_pitches, _ = librosa.piptrack(y=ref_y, sr=sr)\n",
    "        user_pitch_pattern = [np.max(user_pitches[:, i]) for i in range(len(user_pitches[0])) if np.max(user_pitches[:, i]) > 0]\n",
    "        ref_pitch_pattern = [np.max(ref_pitches[:, i]) for i in range(len(ref_pitches[0])) if np.max(ref_pitches[:, i]) > 0]\n",
    "\n",
    "        min_length = min(len(user_pitch_pattern), len(ref_pitch_pattern))\n",
    "        pitch_contour_similarity = np.corrcoef(user_pitch_pattern[:min_length], ref_pitch_pattern[:min_length])[0, 1]\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_contour_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': pitch_contour_similarity\n",
    "        })\n",
    "\n",
    "        user_times = librosa.frames_to_time(range(len(user_pitches[0])), sr=sr)\n",
    "        ref_times = librosa.frames_to_time(range(len(ref_pitches[0])), sr=sr)\n",
    "        user_intervals = np.diff(user_times)\n",
    "        ref_intervals = np.diff(ref_times)\n",
    "        rhythm_similarity = np.abs(user_intervals[:min_length] - ref_intervals[:min_length]).mean()\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rhythm_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': rhythm_similarity\n",
    "        })\n",
    "\n",
    "        # 2) 정확성 (Pitch 평균, RMS)\n",
    "        user_avg_pitch = np.mean(user_pitch_pattern)\n",
    "        ref_avg_pitch = np.mean(ref_pitch_pattern)\n",
    "        pitch_difference = np.abs(user_avg_pitch - ref_avg_pitch)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_difference',\n",
    "            'category': '정확성',\n",
    "            'value': pitch_difference\n",
    "        })\n",
    "\n",
    "        user_rms = librosa.feature.rms(y=user_y).mean()\n",
    "        ref_rms = librosa.feature.rms(y=ref_y).mean()\n",
    "        rms_difference = np.abs(user_rms - ref_rms)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rms_difference',\n",
    "            'category': '정확성',\n",
    "            'value': rms_difference\n",
    "        })\n",
    "\n",
    "        # 3) 유창성 (Speech Rate, Pauses)\n",
    "        user_duration = librosa.get_duration(y=user_y, sr=sr)\n",
    "        ref_duration = librosa.get_duration(y=ref_y, sr=sr)\n",
    "        speech_rate_difference = np.abs((len(user_pitch_pattern) / user_duration) - (len(ref_pitch_pattern) / ref_duration))\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'speech_rate_difference',\n",
    "            'category': '유창성',\n",
    "            'value': speech_rate_difference\n",
    "        })\n",
    "\n",
    "        pause_threshold = 0.3\n",
    "        user_pauses = len([i for i in np.diff(user_times) if i > pause_threshold])\n",
    "        ref_pauses = len([i for i in np.diff(ref_times) if i > pause_threshold])\n",
    "        pause_difference = np.abs(user_pauses - ref_pauses)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pause_difference',\n",
    "            'category': '유창성',\n",
    "            'value': pause_difference\n",
    "        })\n",
    "\n",
    "        # 4) 완전성 (Word Omission)\n",
    "        word_omission = 0  # Simplified for brevity\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'word_omission',\n",
    "            'category': '완전성',\n",
    "            'value': word_omission\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'error',\n",
    "            'category': 'Error',\n",
    "            'value': str(e)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 메인 함수\n",
    "def main(reference_folder, user_folder):\n",
    "    all_results = []\n",
    "\n",
    "    for ref_file in os.listdir(reference_folder):\n",
    "        if ref_file.endswith(\".wav\"):\n",
    "            ref_path = os.path.join(reference_folder, ref_file)\n",
    "            \n",
    "            for user_file in os.listdir(user_folder):\n",
    "                if user_file.endswith(\".wav\"):\n",
    "                    user_path = os.path.join(user_folder, user_file)\n",
    "                    all_results.extend(evaluate_audio(user_path, ref_path))\n",
    "\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(df)\n",
    "\n",
    "# 실행\n",
    "reference_folder = \"./test_audio/adult_tts\"\n",
    "user_folder = \"./test_audio/child_original\"\n",
    "main(reference_folder, user_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Results saved to ./evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import speech\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Google Cloud Speech-to-Text API 설정\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"KEY_PATH\")\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_audio(user_file, reference_file):\n",
    "    results = []\n",
    "    try:\n",
    "        # Load audio files\n",
    "        user_y, sr = librosa.load(user_file)\n",
    "        ref_y, _ = librosa.load(reference_file)\n",
    "\n",
    "        # 1) 음율적 오류 (Pitch, Rhythm)\n",
    "        user_pitches, _ = librosa.piptrack(y=user_y, sr=sr)\n",
    "        ref_pitches, _ = librosa.piptrack(y=ref_y, sr=sr)\n",
    "        user_pitch_pattern = [np.max(user_pitches[:, i]) for i in range(len(user_pitches[0])) if np.max(user_pitches[:, i]) > 0]\n",
    "        ref_pitch_pattern = [np.max(ref_pitches[:, i]) for i in range(len(ref_pitches[0])) if np.max(ref_pitches[:, i]) > 0]\n",
    "\n",
    "        min_length = min(len(user_pitch_pattern), len(ref_pitch_pattern))\n",
    "        pitch_contour_similarity = np.corrcoef(user_pitch_pattern[:min_length], ref_pitch_pattern[:min_length])[0, 1]\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_contour_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': pitch_contour_similarity\n",
    "        })\n",
    "\n",
    "        user_times = librosa.frames_to_time(range(len(user_pitches[0])), sr=sr)\n",
    "        ref_times = librosa.frames_to_time(range(len(ref_pitches[0])), sr=sr)\n",
    "        user_intervals = np.diff(user_times)\n",
    "        ref_intervals = np.diff(ref_times)\n",
    "        rhythm_similarity = np.abs(user_intervals[:min_length] - ref_intervals[:min_length]).mean()\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rhythm_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': rhythm_similarity\n",
    "        })\n",
    "\n",
    "        # 2) 정확성 (Pitch 평균, RMS)\n",
    "        user_avg_pitch = np.mean(user_pitch_pattern)\n",
    "        ref_avg_pitch = np.mean(ref_pitch_pattern)\n",
    "        pitch_difference = np.abs(user_avg_pitch - ref_avg_pitch)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_difference',\n",
    "            'category': '정확성',\n",
    "            'value': pitch_difference\n",
    "        })\n",
    "\n",
    "        user_rms = librosa.feature.rms(y=user_y).mean()\n",
    "        ref_rms = librosa.feature.rms(y=ref_y).mean()\n",
    "        rms_difference = np.abs(user_rms - ref_rms)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rms_difference',\n",
    "            'category': '정확성',\n",
    "            'value': rms_difference\n",
    "        })\n",
    "\n",
    "        # 3) 유창성 (Speech Rate, Pauses)\n",
    "        user_duration = librosa.get_duration(y=user_y, sr=sr)\n",
    "        ref_duration = librosa.get_duration(y=ref_y, sr=sr)\n",
    "        speech_rate_difference = np.abs((len(user_pitch_pattern) / user_duration) - (len(ref_pitch_pattern) / ref_duration))\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'speech_rate_difference',\n",
    "            'category': '유창성',\n",
    "            'value': speech_rate_difference\n",
    "        })\n",
    "\n",
    "        pause_threshold = 0.3\n",
    "        user_pauses = len([i for i in np.diff(user_times) if i > pause_threshold])\n",
    "        ref_pauses = len([i for i in np.diff(ref_times) if i > pause_threshold])\n",
    "        pause_difference = np.abs(user_pauses - ref_pauses)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pause_difference',\n",
    "            'category': '유창성',\n",
    "            'value': pause_difference\n",
    "        })\n",
    "\n",
    "        # 4) 완전성 (Word Omission)\n",
    "        word_omission = 0  # Simplified for brevity\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'word_omission',\n",
    "            'category': '완전성',\n",
    "            'value': word_omission\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'error',\n",
    "            'category': 'Error',\n",
    "            'value': str(e)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 메인 함수\n",
    "def main(reference_folder, user_folder, output_csv):\n",
    "    all_results = []\n",
    "\n",
    "    for ref_file in os.listdir(reference_folder):\n",
    "        if ref_file.endswith(\".wav\"):\n",
    "            ref_path = os.path.join(reference_folder, ref_file)\n",
    "            \n",
    "            for user_file in os.listdir(user_folder):\n",
    "                if user_file.endswith(\".wav\"):\n",
    "                    user_path = os.path.join(user_folder, user_file)\n",
    "                    all_results.extend(evaluate_audio(user_path, ref_path))\n",
    "\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # CSV로 저장\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[INFO] Results saved to {output_csv}\")\n",
    "\n",
    "# 실행\n",
    "reference_folder = \"./test_audio/adult_tts\"\n",
    "user_folder = \"./test_audio/adult_original\"\n",
    "output_csv = \"./evaluation_results.csv\"\n",
    "main(reference_folder, user_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Results saved to ./evaluation_results_child.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import speech\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Google Cloud Speech-to-Text API 설정\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getenv(\"KEY_PATH\")\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_audio(user_file, reference_file):\n",
    "    results = []\n",
    "    try:\n",
    "        # Load audio files\n",
    "        user_y, sr = librosa.load(user_file)\n",
    "        ref_y, _ = librosa.load(reference_file)\n",
    "\n",
    "        # 1) 음율적 오류 (Pitch, Rhythm)\n",
    "        user_pitches, _ = librosa.piptrack(y=user_y, sr=sr)\n",
    "        ref_pitches, _ = librosa.piptrack(y=ref_y, sr=sr)\n",
    "        user_pitch_pattern = [np.max(user_pitches[:, i]) for i in range(len(user_pitches[0])) if np.max(user_pitches[:, i]) > 0]\n",
    "        ref_pitch_pattern = [np.max(ref_pitches[:, i]) for i in range(len(ref_pitches[0])) if np.max(ref_pitches[:, i]) > 0]\n",
    "\n",
    "        min_length = min(len(user_pitch_pattern), len(ref_pitch_pattern))\n",
    "        pitch_contour_similarity = np.corrcoef(user_pitch_pattern[:min_length], ref_pitch_pattern[:min_length])[0, 1]\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_contour_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': pitch_contour_similarity\n",
    "        })\n",
    "\n",
    "        user_times = librosa.frames_to_time(range(len(user_pitches[0])), sr=sr)\n",
    "        ref_times = librosa.frames_to_time(range(len(ref_pitches[0])), sr=sr)\n",
    "        user_intervals = np.diff(user_times)\n",
    "        ref_intervals = np.diff(ref_times)\n",
    "        rhythm_similarity = np.abs(user_intervals[:min_length] - ref_intervals[:min_length]).mean()\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rhythm_similarity',\n",
    "            'category': '음율적 오류',\n",
    "            'value': rhythm_similarity\n",
    "        })\n",
    "\n",
    "        # 2) 정확성 (Pitch 평균, RMS)\n",
    "        user_avg_pitch = np.mean(user_pitch_pattern)\n",
    "        ref_avg_pitch = np.mean(ref_pitch_pattern)\n",
    "        pitch_difference = np.abs(user_avg_pitch - ref_avg_pitch)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pitch_difference',\n",
    "            'category': '정확성',\n",
    "            'value': pitch_difference\n",
    "        })\n",
    "\n",
    "        user_rms = librosa.feature.rms(y=user_y).mean()\n",
    "        ref_rms = librosa.feature.rms(y=ref_y).mean()\n",
    "        rms_difference = np.abs(user_rms - ref_rms)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'rms_difference',\n",
    "            'category': '정확성',\n",
    "            'value': rms_difference\n",
    "        })\n",
    "\n",
    "        # 3) 유창성 (Speech Rate, Pauses)\n",
    "        user_duration = librosa.get_duration(y=user_y, sr=sr)\n",
    "        ref_duration = librosa.get_duration(y=ref_y, sr=sr)\n",
    "        speech_rate_difference = np.abs((len(user_pitch_pattern) / user_duration) - (len(ref_pitch_pattern) / ref_duration))\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'speech_rate_difference',\n",
    "            'category': '유창성',\n",
    "            'value': speech_rate_difference\n",
    "        })\n",
    "\n",
    "        pause_threshold = 0.3\n",
    "        user_pauses = len([i for i in np.diff(user_times) if i > pause_threshold])\n",
    "        ref_pauses = len([i for i in np.diff(ref_times) if i > pause_threshold])\n",
    "        pause_difference = np.abs(user_pauses - ref_pauses)\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'pause_difference',\n",
    "            'category': '유창성',\n",
    "            'value': pause_difference\n",
    "        })\n",
    "\n",
    "        # 4) 완전성 (Word Omission)\n",
    "        word_omission = 0  # Simplified for brevity\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'word_omission',\n",
    "            'category': '완전성',\n",
    "            'value': word_omission\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            'user_file': os.path.basename(user_file),\n",
    "            'reference_file': os.path.basename(reference_file),\n",
    "            'evaluation_metric': 'error',\n",
    "            'category': 'Error',\n",
    "            'value': str(e)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 메인 함수\n",
    "def main(reference_folder, user_folder, output_csv):\n",
    "    all_results = []\n",
    "\n",
    "    for ref_file in os.listdir(reference_folder):\n",
    "        if ref_file.endswith(\".wav\"):\n",
    "            ref_path = os.path.join(reference_folder, ref_file)\n",
    "            \n",
    "            for user_file in os.listdir(user_folder):\n",
    "                if user_file.endswith(\".wav\"):\n",
    "                    user_path = os.path.join(user_folder, user_file)\n",
    "                    all_results.extend(evaluate_audio(user_path, ref_path))\n",
    "\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # CSV로 저장\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[INFO] Results saved to {output_csv}\")\n",
    "\n",
    "# 실행\n",
    "reference_folder = \"./test_audio/adult_tts\"\n",
    "user_folder = \"./test_audio/child_original\"\n",
    "output_csv = \"./evaluation_results_child.csv\"\n",
    "main(reference_folder, user_folder, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epa_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
