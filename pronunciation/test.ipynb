{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [원어민vs사람 음성 유사도 평가 항목]\n",
    "# 피치: 음성의 높낮이(억양, 리듬) 평가\n",
    "        # 의미 전달(감정 표현/강조)\n",
    "# 포먼트: 모음 발음의 정확도(각 모음은 고유한 포먼트 주파수 패턴을 가짐)\n",
    "# MFCC: 모음 뿐만 아니라 모든 음소의 발음의 명확성/구별성(발음의 청각적 유사성 측정; 사람의 청각 특성에 맞춰진 Mel 주파수 스케일 사용)\n",
    "# 타이밍: 발화 속도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [유사도 평가시, 코사인 vs 유클리드 거리]\n",
    "# 코사인 유사도와 유클리드 거리 중 코사인 유사도를 사용하는 것이 발음 비교에서는 더 적합할 수 있음\n",
    "# 코사인 유사도는 두 벡터 간의 방향성(패턴의 유사성)을 평가: 크기보다 형태(발음 패턴의 유사성)를 평가할 때 유리함\n",
    "# 특히 피치나 포먼트, MFCC는 주파수나 스펙트럼 패턴을 다루므로 코사인 유사도를 활용하는 것이 일반적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![발음 유사도 채점기준표](./스크린샷%202024-11-12%20124814.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import parselmouth\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from praatio import textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 및 MFA 결과 TextGrid 파일 경로 설정\n",
    "audio_path_1 = \"./audio/example_tts.wav\"  # 원어민 음성\n",
    "audio_path_2 = \"./audio/user_audio.wav\"  # 사용자 음성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2496835917.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    mfa align ./audio ./text ./english.dict ./english ./textgrid\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TextGrid 파일 생성\n",
    "mfa align ./audio ./text ./english.dict ./english ./textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextGrid 파일 경로 설정 (MFA 실행 후 생성된 파일 경로)\n",
    "textgrid_path_1 = \"./textgrid/example_tts.TextGrid\"\n",
    "textgrid_path_2 = \"./textgrid/user_audio.TextGrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\epa\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Whisper 모델 불러오기\n",
    "whisper_model = whisper.load_model(\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성을 텍스트로 변환\n",
    "def transcribe_audio(audio_path):\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    print(\"음성 텍스트 변환 결과:\", result[\"text\"])\n",
    "    return result[\"text\"]\n",
    "\n",
    "# 음성 텍스트 변환\n",
    "text1 = transcribe_audio(audio_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextGrid 파일 불러오기\n",
    "tg_1 = tgio.openTextgrid(textgrid_path_1, includeEmptyIntervals=False)\n",
    "tg_2 = tgio.openTextgrid(textgrid_path_2, includeEmptyIntervals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피치, 포먼트, MFCC 계산 함수\n",
    "def analyze_features(audio_path, start, end):\n",
    "    sound = parselmouth.Sound(audio_path)\n",
    "    segment = sound.extract_part(from_time=start, to_time=end)\n",
    "    \n",
    "    # 피치\n",
    "    pitch = segment.to_pitch().selected_array['frequency']\n",
    "    \n",
    "    # 포먼트\n",
    "    formant = segment.to_formant_burg()\n",
    "    formant_values = [formant.get_value_at_time(f, (start + end) / 2) for f in range(1, 4)]  # 포먼트 F1, F2, F3\n",
    "    \n",
    "    # MFCC\n",
    "    y_segment, _ = librosa.load(audio_path, sr=sr1, offset=start, duration=(end - start))\n",
    "    mfcc = librosa.feature.mfcc(y=y_segment, sr=sr1, n_mfcc=13)\n",
    "    \n",
    "    return pitch, formant_values, mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산 함수\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if len(vec1) != len(vec2):\n",
    "        return None\n",
    "    return 1 - cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 및 음소별 비교 함수\n",
    "def compare_features(tg1, tg2, word_tier=\"words\", phone_tier=\"phones\"):\n",
    "    word_intervals_1 = tg1.tierDict[word_tier].entryList\n",
    "    word_intervals_2 = tg2.tierDict[word_tier].entryList\n",
    "    phone_intervals_1 = tg1.tierDict[phone_tier].entryList\n",
    "    phone_intervals_2 = tg2.tierDict[phone_tier].entryList\n",
    "    \n",
    "    for word_index, (word1, word2) in enumerate(zip(word_intervals_1, word_intervals_2)):\n",
    "        word_label1, word_start1, word_end1 = word1.label, word1.start, word1.end\n",
    "        word_label2, word_start2, word_end2 = word2.label, word2.start, word2.end\n",
    "        \n",
    "        # 동일한 단어인지 확인\n",
    "        if word_label1 == word_label2:\n",
    "            print(f\"\\n단어 '{word_label1}' (구간 {word_index + 1}) 비교:\")\n",
    "            \n",
    "            # 단어 구간 내 음소별 비교\n",
    "            for i, (phone1, phone2) in enumerate(zip(phone_intervals_1, phone_intervals_2)):\n",
    "                # 단어 구간 내 음소만 필터링\n",
    "                if phone1.start >= word_start1 and phone1.end <= word_end1 and \\\n",
    "                   phone2.start >= word_start2 and phone2.end <= word_end2:\n",
    "                    \n",
    "                    # 음소 정보\n",
    "                    phone_label1, start1, end1 = phone1.label, phone1.start, phone1.end\n",
    "                    phone_label2, start2, end2 = phone2.label, phone2.start, phone2.end\n",
    "                    \n",
    "                    # 동일한 음소인지 확인\n",
    "                    if phone_label1 == phone_label2:\n",
    "                        print(f\"  음소 '{phone_label1}' (단어 '{word_label1}' 내 음소 {i + 1}) 비교:\")\n",
    "                        \n",
    "                        # 각 음성의 피치, 포먼트, MFCC 계산\n",
    "                        pitch1, formant1, mfcc1 = analyze_features(audio_path_1, start1, end1)\n",
    "                        pitch2, formant2, mfcc2 = analyze_features(audio_path_2, start2, end2)\n",
    "                        \n",
    "                        # 코사인 유사도 계산 및 출력\n",
    "                        pitch_similarity = cosine_similarity(pitch1, pitch2)\n",
    "                        formant_similarity = cosine_similarity(formant1, formant2)\n",
    "                        mfcc_similarity = cosine_similarity(np.mean(mfcc1, axis=1), np.mean(mfcc2, axis=1))\n",
    "                        \n",
    "                        # 타이밍 차이\n",
    "                        timing_difference = (end2 - start2) - (end1 - start1)\n",
    "                        \n",
    "                        # 결과 출력\n",
    "                        print(f\"    피치 유사도: {pitch_similarity:.2f}\")\n",
    "                        print(f\"    포먼트 유사도: {formant_similarity:.2f}\")\n",
    "                        print(f\"    MFCC 유사도: {mfcc_similarity:.2f}\")\n",
    "                        print(f\"    타이밍 차이: {timing_difference:.2f} 초\")\n",
    "                    else:\n",
    "                        print(f\"  Warning: 음소가 일치하지 않습니다 ('{phone_label1}' vs '{phone_label2}')\")\n",
    "        else:\n",
    "            print(f\"Warning: 단어가 일치하지 않습니다 ('{word_label1}' vs '{word_label2}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 및 음소별 비교 실행\n",
    "compare_features(tg_1, tg_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
